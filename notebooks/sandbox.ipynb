{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767838c2-5a4e-40a9-a0d5-ecfc9d17bb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce9ad2-a313-46a0-a694-e0ad145056da",
   "metadata": {},
   "source": [
    "## load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be21ba5a-da46-464c-8aae-592a27d07c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at llm-jp/llm-jp-3-1.8b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "from peft.peft_model import PeftModelForSequenceClassification\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llm-jp/llm-jp-3-1.8b\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"llm-jp/llm-jp-3-1.8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cebde34-1839-4263-98f0-fa25421cfbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2a49ab-73c3-4466-a04b-3f6be82c0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchedPeftModelForSequenceClassification(PeftModelForSequenceClassification):\n",
    "    def add_adapter(self, adaper_name, peft_config, low_cpu_mem_usage: bool = False):\n",
    "        super().add_adapter(adapter_name, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "252315a2-83ce-4e5f-963f-3b8064f6e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be25121d-6fd8-4967-8cd4-dad9dab39576",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PatchedPeftModelForSequenceClassification(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159790d1-c524-4080-92b4-a640fc0b3ee4",
   "metadata": {},
   "source": [
    "## load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee9ce9f-1da7-4805-aed6-79fa5ec21188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3c4626-3197-4885-a6c2-27ea0d75b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"../data/train.tsv\",\n",
    "        \"valid\": \"../data/valid.tsv\",\n",
    "        \"test\": \"../data/test.tsv\",\n",
    "    },\n",
    "    delimiter=\"\\t\",\n",
    ").rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9225b-024e-4b03-8b47-cdbf65104a37",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "680324fe-00db-4018-ad16-e8b045620d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20241201051941'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "yyyymmddhhmmss = \"{:%Y%m%d%H%M%S}\".format(datetime.datetime.now())\n",
    "yyyymmddhhmmss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49bbe960-d060-4245-8846-8c2f823f6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        encoding = self.tokenizer(\n",
    "            [ex[\"poem\"] for ex in examples],\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=200,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "            \"labels\": torch.tensor([ex[\"labels\"] for ex in examples]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fa8c9d2-bd29-499b-b29e-8b3a51e4df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "roc_auc_evaluate = evaluate.load(\"roc_auc\")\n",
    "acc_evaluate = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = map(torch.tensor, eval_pred)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)[:, 1]  # label=1„ÅÆÁ¢∫Áéá\n",
    "    pred_labels = torch.argmax(logits, dim=1)  # ‰∫àÊ∏¨„É©„Éô„É´\n",
    "    return {\n",
    "        **roc_auc_evaluate.compute(prediction_scores=probs, references=labels),\n",
    "        **acc_evaluate.compute(predictions=pred_labels, references=labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e226ab5-4189-4462-ba18-0e3cc0bd3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../results/{yyyymmddhhmmss}\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=1.0,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af756c04-c0b6-45e2-8d4b-79e20ed63736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22616/278244700.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1620/1620 01:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.162300</td>\n",
       "      <td>0.371995</td>\n",
       "      <td>0.949013</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.490962</td>\n",
       "      <td>0.962993</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.310664</td>\n",
       "      <td>0.967105</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.382418</td>\n",
       "      <td>0.972862</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.374812</td>\n",
       "      <td>0.972862</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.381211</td>\n",
       "      <td>0.978618</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.311052</td>\n",
       "      <td>0.978618</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.325294</td>\n",
       "      <td>0.978618</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.312975</td>\n",
       "      <td>0.978618</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.313705</td>\n",
       "      <td>0.978618</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1620, training_loss=0.34542546684359327, metrics={'train_runtime': 99.7472, 'train_samples_per_second': 16.241, 'train_steps_per_second': 16.241, 'total_flos': 195863262167040.0, 'train_loss': 0.34542546684359327, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=TokenizeCollator(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d22c7a8-7f8c-406f-b5f2-665231fd47f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': np.float64(0.9835526315789473), 'accuracy': 0.9074074074074074}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics((trainer.predict(ds[\"test\"]).predictions, ds[\"test\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154ea65-7f85-48fa-94a7-5c0ad4177fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
